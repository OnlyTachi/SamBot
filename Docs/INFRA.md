
# üìë Guia T√©cnico: Infraestrutura Local e Arquitetura SamBot

Este documento fornece uma vis√£o consolidada da camada de processamento local do projeto **SamBot**. Ele detalha como o **Ollama** atua como o motor de intelig√™ncia, integrando uma rede h√≠brida de hardware para equilibrar performance, economia de recursos e privacidade.

---

## 1. Vis√£o Geral da Arquitetura

A SamBot utiliza uma abordagem de **IA H√≠brida**. As tarefas s√£o distribu√≠das entre um servidor de baixa pot√™ncia (Acer ES1) e uma workstation potente (Xeon + RX 580), conectadas via rede Mesh (**Tailscale**).

### Por que esta arquitetura?

* **Privacidade e Custo:** O processamento local via Ollama reduz a depend√™ncia de APIs pagas e mant√©m dados sens√≠veis fora da nuvem.
* **Failover Inteligente:** Capacidade de alternar entre modelos locais e o Gemini (nuvem) caso algum n√≥ da rede fique offline.
* **Otimiza√ß√£o de Hardware:** Distribui√ß√£o de carga conforme a complexidade da tarefa.

---

## 2. A "Stack" de Hardware

### üöÄ N√≥ de Alta Performance (Workstation Principal)

* **SO:** Linux Mint
* **CPU:** Intel Xeon E3-1270
* **RAM:** 24GB RAM
* **GPU:** AMD Radeon RX 580 (8GB VRAM)
* **Fun√ß√£o:** Motor principal de processamento de LLMs pesados com acelera√ß√£o ROCm.

### üîã N√≥ de Baixa Pot√™ncia (Servidor)

* **SO:** Ubuntu Server
* **Dispositivo:** Acer ES1-573 (i3 6006u, 8GB RAM)
* **Fun√ß√£o:** Host do bot, containers Docker e modelos ultra-leves para comandos r√°pidos.

---

## 3. Estrat√©gia de Modelos (LLMs)

O SamBot utiliza tr√™s modelos espec√≠ficos para diferentes prop√≥sitos:

| Modelo | Fun√ß√£o | Localiza√ß√£o | Vantagem |
| --- | --- | --- | --- |
| **Phi-3.5** | L√≥gica e Racioc√≠nio | Workstation (Mint) | Alta precis√£o e velocidade via GPU. |
| **Qwen-2.5 1.5B** | Comandos R√°pidos | Servidor (Ubuntu) | Extremamente leve, evita lat√™ncia. |
| **Nomic-Embed-Text** | Mem√≥ria (Embeddings) | Workstation/Server | Vetoriza√ß√£o para o ChromaDB (RAG). |

---

## 4. Instala√ß√£o e Configura√ß√£o

### üõ†Ô∏è Configura√ß√£o no Linux Mint (Acelera√ß√£o GPU RX 580)

No Linux, o Ollama utiliza o **ROCm** para acessar o poder da GPU AMD, o que √© significativamente mais est√°vel que as solu√ß√µes Windows para esta placa.

1. **Instalar drivers ROCm:** Certifique-se de que os drivers de v√≠deo e suporte ROCm est√£o instalados no Mint.
2. **Instalar Ollama:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

3. **Baixar Modelos:**
```bash
ollama run phi3.5:latest
ollama pull nomic-embed-text:latest
```

> **Verifica√ß√£o:** Use `rocm-smi` ou verifique os logs do Ollama para confirmar a detec√ß√£o da RX 580.

*Obs: Isso varia de placa e SO*
### ‚öôÔ∏è Configura√ß√£o no Ubuntu Server (Acer ES1)

1. **Instalar Ollama:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```


2. **Baixar Modelo Leve:**
```bash
ollama run qwen2.5:1.5b
```



---

## 5. Conectividade e Rede Mesh (Tailscale)

Para que o SamBot no servidor Ubuntu consiga "falar" com o Ollama no PC Mint de forma segura pela internet, utilizamos o **Tailscale**.

### Passo a Passo da Conex√£o:

1. **Habilitar Acesso Externo (N√≥ Mint):**
√â necess√°rio configurar o Ollama para aceitar conex√µes fora do localhost. No Linux, edite o servi√ßo do systemd:
* Adicione a vari√°vel de ambiente: `OLLAMA_HOST=0.0.0.0`


2. **Configura√ß√£o no arquivo `.env` do SamBot:**
O bot deve apontar para o IP interno gerado pelo Tailscale para a workstation.
```env
# IP do Tailscale da Workstation Linux Mint
OLLAMA_REMOTE_URL="http://100.x.y.z:11434" 

```
### Extra usando docker (meu caso)


## 1. Subindo o Container com Acesso Externo

Por padr√£o, o Ollama no Docker escuta apenas em `127.0.0.1`. Para liberar o acesso para outros IPs da sua rede local ou Tailscale, voc√™ deve definir a vari√°vel `OLLAMA_HOST`.

### Com Suporte a GPU (Recomendado para sua Workstation)

Se estiver no Linux Mint com drivers ROCm (AMD) ou NVIDIA instalados:

```bash
docker run -d \
  --name ollama \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  --device /dev/kfd --device /dev/dri \
  -e OLLAMA_HOST=0.0.0.0 \
  ollama/ollama

```

### Apenas CPU (Para o seu Acer ES1)

```bash
docker run -d \
  --name ollama \
  -v ollama:/root/.ollama \
  -p 11434:11434 \
  -e OLLAMA_HOST=0.0.0.0 \
  ollama/ollama

```

## 2. Entendendo os Par√¢metros

* **`-p 11434:11434`**: Mapeia a porta interna do container para a porta f√≠sica da sua m√°quina.
* **`-e OLLAMA_HOST=0.0.0.0`**: Esta √© a chave. Ao definir como `0.0.0.0`, voc√™ diz ao Ollama para aceitar conex√µes de qualquer interface de rede, n√£o apenas de dentro do container.
* **`-v ollama:/root/.ollama`**: Garante que os modelos que voc√™ baixar n√£o sejam apagados se o container for reiniciado.

## 3. Configura√ß√£o de CORS (Opcional, mas importante)

Se voc√™ pretende acessar o Ollama atrav√©s de uma interface web (como o Open WebUI) rodando em outro dom√≠nio ou IP, voc√™ tamb√©m precisar√° liberar as origens:

Adicione esta flag ao comando `docker run`:
`-e OLLAMA_ORIGINS="*"`

## 4. Validando o Acesso

Ap√≥s subir o container, verifique se a porta est√° aberta e respondendo:

1. **Localmente:** `curl http://localhost:11434`
2. **De outro PC na rede:** `curl http://IP_DA_MAQUINA:11434`

Se o retorno for `Ollama is running`, a configura√ß√£o foi bem-sucedida.

---

## 5. Exemplo com Docker Compose (Mais organizado)

Para facilitar a manuten√ß√£o no seu servidor, use um arquivo `docker-compose.yml`:

```yaml
services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

volumes:
  ollama_data:

```

### Como aplicar?

1. Salve o c√≥digo acima em um arquivo chamado `docker-compose.yml`.
2. Rode o comando: `docker compose up -d`.

---

**Dica para sua rede Mesh:** Na SamBot, agora voc√™ pode apontar o `OLLAMA_REMOTE_URL` para o IP do Tailscale da m√°quina onde este container est√° rodando, seguido da porta `:11434`.


---

## 6. Dicas de Performance e Manuten√ß√£o

> [!IMPORTANT]
> **Gest√£o de VRAM:** A RX 580 possui 8GB de VRAM. Modelos como o Phi-3.5 (3.8B) cabem inteiros na mem√≥ria, garantindo respostas instant√¢neas. Evite rodar m√∫ltiplos modelos grandes simultaneamente para n√£o for√ßar o uso de RAM (swap).

> [!TIP]
> **Persist√™ncia de Servi√ßo:** No Linux Mint, configure o `systemd` para garantir que o Ollama inicie automaticamente no boot, permitindo que o SamBot funcione mesmo que a interface gr√°fica n√£o esteja logada.

> [!NOTE]
> **Mem√≥ria Infinita (RAG):** O uso do `nomic-embed-text` em conjunto com o ChromaDB permite que o bot consulte o hist√≥rico de conversas do Discord como uma base de conhecimento, criando uma sensa√ß√£o de continuidade e mem√≥ria de longo prazo.

---

**Documenta√ß√£o atualizada em:** Novembro de 2025
